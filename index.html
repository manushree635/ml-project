<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Social Media and Mental Health Analysis</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>

    <h1>Social Media and Mental Health Analysis</h1>
    <h3>CS 7641 Project Proposal - Team 39</h3>

    <div class="section">
        <h2>Problem Definition</h2>
        <h3>Problem:</h3>
        <p>Given that social media platforms have a big impact on our mental health, is there a way to determine a user's mental health status based on factors such as use time?</p>
        
        <h3>Motivation:</h3>
        <ul>
            <li>Social media has made a controversial impact on the lives of many people around the world</li>
            <li>Some users have been able to grow their brand and image</li>
            <li>Others have negative experiences in regards to mental health
                <ul>
                    <li>E.g. Some teens reported bullying using Instagram</li>
                </ul>
            </li>
        </ul>
    </div>

    <div class="section">
        <h2>Literature Review</h2>
        <ul>
            <li><strong>ASSOCIATION BETWEEN SOCIAL MEDIA USE AND DEPRESSION AMONG U.S. YOUNG ADULTS:</strong> 1787 Adults were surveyed about how much they use social media and their level of depression. Findings showed a strong correlation between the two.</li>
            <li><strong>Predicting Depression via Social Media:</strong> Posts of social media users who reported symptoms of depression were collected, and used to predict depression based on posting patterns. Findings showed it may be possible to predict depression based on post content and frequency.</li>
            <li><strong>The Relationship Between Facebook Use and Well-Being:</strong> Activities of Facebook users were used to measure user's psychological well being. Findings showed that communication with people users were close to in real life directly influenced their perceived well being.</li>
        </ul>
    </div>

    <div class="section">
        <h2>Database Description</h2>
        <p><strong>Link:</strong> <a href="https://www.kaggle.com/datasets/emirhanai/social-media-usage-and-emotional-well-being/data" target="_blank">Kaggle Dataset</a></p>
        <p><strong>Description:</strong> This dataset contains a thousand data points that contains information about users' activity on a social media platform like Instagram. Attributes in this dataset include biological info like age and metric collected info like daily usage time. The dataset's main purpose is centered around how users interaction with a social media platform has an effect on their mental health.</p>
    </div>

    <div class="section">
        <h2>Goals & Expected Results</h2>
        <h3>Expected results:</h3>
        <ul>
            <li>We expect to see a negative effect on mood the higher the amount of time a user spends on social media</li>
            <li>If there is a correlation, we expect to be able to predict a user's mood based on that</li>
        </ul>

        <h3>Goals:</h3>
        <ul>
            <li>Classify mood based on social media usage</li>
            <li>Show correlation between social media usage and mood</li>
        </ul>
    </div>

    <div class="section">
        <h2>Method</h2>

        <h3>Data Processing Methods:</h3>
        <p>An important first step was cleaning our dataset to ensure consistency. We:
        </p>
        <ul>
            <li>Fixed misaligned columns</li>
            <li>Removed empty rows</li>
            <li>Removed duplicate entries</li>
            <li>Filled missing values in numerical columns with the mean</li>
            <li>Verified that categorical data matched predefined categories and removed incorrect values 
                (for example we removed the entry with "Marie" in the Gender column, where the valid categories are "Male," "Female," and "Non-binary").
            </li>
        </ul>
        <p>
            An example of misaligned columns is provided below. This was the biggest issue in the uncleaned dataset - overall, there were 72 of these problems. 
            <div style="text-align: center;">
                <img src="./misaligned columns 1.png" alt="Misaligned columns" style="width: 100%; max-width: 600px; height: auto;">
            </div>
        </p>


        <h3> Dataset Split</h3>
        <p>Our dataset was split into train set and test set with 1000 data points in the train set and 97 data points in the test set. 
        We train our model on the train set and test it on the test set.
        </p>

        <h3> Feature Reduction </h3>
        <p> Given that our dataset had 9 features (not including the y column 'Dominant_Emotion'), we decided to try to use PCA to conduct feature reduction. With feature reduction, we aimed to reduce the size of our training and testing datasets for better performance and reduce chances of overfitting. Steps in this process include: </p>

        <ul>
            <li>Finding the number of principal components (n) to keep, so that they keep at least 95% of the total variance</li>
            <li>Reducing both the training and testing datasets to have n principal components to serve as features and combining it with the y column 'Dominant_Emotion'</li>
            <li>Analyzing how our PCA feature reduction does on a couple of models</li>

        </ul>

        <p>
            Using the graph below, we determined that we could keep 4 Principal Components for every user, effectively reducing our training and testing dataset. This is because we are able to keep at least 95% of the total variance with just 4 Principal Components. After this, we adjusted our training and testing datasets.
            <div style="text-align: center;">
                <img src="./PCA_Graph.png" style="width: 100%; max-width: 600px; height: auto;">
            </div>
            
        </p>
        <p>
            However, our PCA feature reduced datasets aren't working well on well-known ML models. The highest accuracy we got with a model was with k-Nearest Neighbors, but that was 49%. This is really bad and shows the tradeoff between reducing datasets to predicting data points. This point is something to keep in mind when looking at the results of our model with and without PCA.
        </p>

<<<<<<< HEAD
        
        <h3>Machine Learning Algorithm 1 (Supervised Learning):</h3>
=======
        <!-- Adding stuff about models and evaluation metrics... -->
        <!-- 1. Bagging Classifier -->
        <h2>Machine Learning Algorithm 1 (Supervised Learning):</h2>
        <h3>1. Bagging Classifier Implementation</h3>
>>>>>>> 0c7c0fa (adding RF info and making it prettier)
        <ul>
            <li><strong>Bagging Model:</strong> Implemented using <code>BaggingClassifier</code> with <code>DecisionTreeClassifier</code> as the base estimator.</li>
            <li><strong>Hyperparameters:</strong>
                <ul>
                    <li><code>n_estimators=50</code>: Number of decision trees. This is the number of trees in the forest.</li>
                    <li><code>max_samples=0.5</code>: 50% of training data per estimator. This is the number of samples to draw from the training data.</li>
                    <li><code>max_features=0.5</code>: 50% of features per estimator. This is the number of features to consider when looking for the best split.</li>
                    <li><code>bootstrap=True</code>: Enables bootstrapping. Bootstrap is a technique where the data sample is drawn with replacement.</li>
                    <li><code>oob_score=True</code>: Uses out-of-bag samples for accuracy estimation. Out-of-bag samples are the samples that are not used to train the model.</li>
                </ul>
            </li>
            <li><strong>Hyperparameter Search:</strong> Conducted grid search to optimize parameters for the bagging model. The parameters we tuned were:
                <ul>
                    <li><code>n_estimators: [10, 50, 100]</code></li>
                    <li><code>max_samples: [0.5, 0.7, 1.0]</code></li>
                    <li><code>max_features: [0.5, 0.7, 1.0]</code></li>
                    <li><code>bootstrap: [True, False]</code></li>
                    <li><code>bootstrap_features: [True, False]</code></li>
                </ul>
            </li>
        </ul>

        <h3>Evaluation Metrics</h3>
        <ul>
            <li><strong>Accuracy:</strong> The percentage of correctly classified data points.</li>
            <li><strong>Precision:</strong> The percentage of true positives that were correctly classified.</li>
            <li><strong>Recall:</strong> The percentage of true positives that were correctly classified.</li>
            <li><strong>F1-score:</strong> The harmonic mean of precision and recall.</li>
        </ul>
<<<<<<< HEAD
    
            </div>
=======
        <!-- Adding stuff about models and evaluation metrics... -->
        <!-- Random Forrest.. -->
        <h3>2. Random Forest Classifier Implementation</h3>
    
        <div class="implementation-details">
            <h4>Model Configuration</h4>
            <ul>
                <li><strong>Classifier:</strong> <code>RandomForestClassifier</code></li>
                <li><strong>Random State:</strong> 1</li>
                <li><strong>Best Parameters Found:</strong>
                    <ul>
                        <li><code>max_depth</code>: None</li>
                        <li><code>max_features</code>: sqrt</li>
                        <li><code>min_samples_leaf</code>: 1</li>
                        <li><code>min_samples_split</code>: 2</li>
                        <li><code>n_estimators</code>: 200</li>
                    </ul>
                </li>
            </ul>
        </div>

        <div class="hyperparameters">
            <h4>Grid Search Parameters</h4>
            <table>
                <tr><th>Parameter</th><th>Values Tested</th></tr>
                <tr><td>n_estimators</td><td>[100, 200, 300]</td></tr>
                <tr><td>max_depth</td><td>[None, 10, 20, 30]</td></tr>
                <tr><td>min_samples_split</td><td>[2, 5, 10]</td></tr>
                <tr><td>min_samples_leaf</td><td>[1, 2, 4]</td></tr>
                <tr><td>max_features</td><td>['sqrt', 'log2']</td></tr>
            </table>
        </div>
    </div>
>>>>>>> 0c7c0fa (adding RF info and making it prettier)

    <div class="section">
        <h2>Results</h2>
        <h3>1. Bagging Classifier: </h3>
        We evaluate our model on the test set and get the following results in the table below for the bagging model with and without pca:

        <table>
            <thead>
                <tr>
                    <th>Metric</th>
                    <th>Bagging Model</th>
                    <th>Bagging Model with PCA</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Accuracy</td>
                    <td>0.96</td>
                    <td>0.43</td>
                </tr>
                <tr>
                    <td>Precision</td>
                    <td>0.95</td>
                    <td>0.42</td>
                </tr>
                <tr>
                    <td>Recall</td>
                    <td>0.96</td>
                    <td>0.43</td>
                </tr>
                <tr>
                    <td>F1-score</td>
                    <td>0.96</td>
                    <td>0.43</td>
                </tr>
            </tbody>
        </table>
        

        <h3>Visualizations</h3>
        <p>We also created a confusion matrix for the bagging model with and without pca to visualize the results.</p>
        <img src="./confusion_matrix_bagging.png" alt="Confusion Matrix of Bagging Model">
        <img src="./confusion_matrix_bagging_without_pca.png" alt="Confusion Matrix of Bagging Model without PCA">

        <h3>Discussion</h3>
        <ul>
            <li>Our model has accuracy of 0.96. This is reasonable because it means that our model is able to correctly classify 96% of the data points which is around 93 samples out of 97.</li>
            <li>We see that the bagging model with pca has a lower accuracy than the bagging model without pca. This is likely because the pca reduces the number of features and removes some important information.</li>
        </ul>

        <!-- Random Forrest CLassifier.... -->
        <h3>2. Random Forrest Classifier: </h3>
        We evaluate our model on the test set and get the following results in the table below for the Random Forrest model with and without pca:

        <table class="metrics-table">
        <thead>
            <tr>
                <th style="width: 25%">Metric</th>
                <th style="width: 37.5%">Random Forest Model</th>
                <th style="width: 37.5%">Random Forest Model with PCA</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Accuracy</td>
                <td class="metric-value">0.958</td>
                <td class="metric-value metric-warning">0.440</td>
            </tr>
            <tr>
                <td>Precision</td>
                <td class="metric-value">0.960</td>
                <td class="metric-value metric-warning">0.420</td>
            </tr>
            <tr>
                <td>Recall</td>
                <td class="metric-value">0.960</td>
                <td class="metric-value metric-warning">0.430</td>
            </tr>
            <tr>
                <td>F1-score</td>
                <td class="metric-value">0.960</td>
                <td class="metric-value metric-warning">0.430</td>
            </tr>
        </tbody>
    </table>
        
        <h3>Classification Report</h3>
        <table class="classification-report">
            <tr><th>Class</th><th>Precision</th><th>Recall</th><th>F1-Score</th><th>Support</th></tr>
            <tr><td>0.0</td><td>0.93</td><td>1.00</td><td>0.97</td><td>14</td></tr>
            <tr><td>1.0</td><td>0.90</td><td>1.00</td><td>0.95</td><td>9</td></tr>
            <tr><td>2.0</td><td>1.00</td><td>1.00</td><td>1.00</td><td>25</td></tr>
            <tr><td>3.0</td><td>1.00</td><td>0.95</td><td>0.97</td><td>20</td></tr>
            <tr><td>4.0</td><td>0.93</td><td>0.93</td><td>0.93</td><td>15</td></tr>
            <tr><td>5.0</td><td>0.92</td><td>0.86</td><td>0.89</td><td>14</td></tr>
        </table>

        <h3>Visualizations</h3>
        <p>We also created a confusion matrix for the bagging model with and without pca to visualize the results.</p>
        <img src="./rf_confusion_matrix_sans_PCA.png" alt="Confusion Matrix of Random Forrest Model without PCA">
        <!-- <img src="./confusion_matrix_bagging_without_pca.png" alt="Confusion Matrix of Bagging Model without PCA"> -->

        <h3>Discussion</h3>
        <ul>
            <li>Our model has accuracy of 0.96. This is reasonable because it means that our model is able to correctly classify 96% of the data points which is around 93 samples out of 97.</li>
        </ul>

        
        
    </div>

    <div class="section">
        <h2>Next Steps</h2>
        <p>This list is a couple of ideas for what our group could possibly do between now and the final presentation:</p>
            <ul>
                <li>Consider using more advanced ensemble methods such as boosting or stacking.</li>
                <li>Explore additional feature selection or transformations that might improve model performance.</li>
                <li>Combine multiple models to create a more robust ensemble, potentially improving accuracy and generalization.</li>
                <li>Research into feature reduction techniques that don't affect our models' accuracy too much, unlike PCA.</li>
            </ul>
    </div>

    <div class="section">
        <h2>References</h2>
        <ol>
            <li>L. yi Lin et al., "Association between Social Media Use and Depression among US Young Adults," Depression and Anxiety, vol. 33, no. 4, pp. 323–331, Jan. 2016, doi: <a href="https://doi.org/10.1002/da.22466" target="_blank">https://doi.org/10.1002/da.22466</a></li>
            <li>M. De Choudhury, M. Gamon, S. Counts, and E. Horvitz, "Predicting Depression via Social Media," Proceedings of the International AAAI Conference on Web and Social Media, vol. 7, no. 1, pp. 128–137, Aug. 2021, doi: <a href="https://doi.org/10.1609/icwsm.v7i1.14432" target="_blank">https://doi.org/10.1609/icwsm.v7i1.14432</a></li>
            <li>M. Burke and R. E. Kraut, "The Relationship Between Facebook Use and Well-Being Depends on Communication Type and Tie Strength," Journal of Computer-Mediated Communication, vol. 21, no. 4, pp. 265–281, Jul. 2016, doi: <a href="https://doi.org/10.1111/jcc4.12162" target="_blank">https://doi.org/10.1111/jcc4.12162</a></li>
        </ol>
    </div>

    <div class="section">
        <h2>Important Links</h2>
        <ul class="links-list">
            <li>
                <strong>GitHub Repository:</strong> 
                <a href="https://github.gatech.edu/aveal6/CS7641_Project" target="_blank">CS7641 Project Repository</a>
            </li>
            <li>
                <strong>Project Timeline:</strong> 
                <a href="https://docs.google.com/spreadsheets/d/14Z7yvJM7WWaoWD7GeCjrhkWzRf-Ky8z5/edit?usp=sharing&ouid=109571213662033046175&rtpof=true&sd=true" target="_blank">Gantt Chart</a>
            </li>
            <li>
                <strong>Video Presentation:</strong> 
                <a href="https://youtu.be/jlIhNwVOmtI" target="_blank">YouTube Video</a>
            </li>
        </ul>
    </div>

    <div class="section">
        <h2>Team Contributions</h2>
        <table class="contribution-table">
            <thead>
                <tr>
                    <th>Team Member</th>
                    <th>Contributions</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Alex</td>
                    <td>
                        <ul>
                            <li>Data Cleaning</li>
                            <li>Update Github Page</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Vineeth</td>
                    <td>
                        <ul>
                            <li>Feature Reduction</li>
                            <li>Update Github Page</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Austin</td>
                    <td>
                        <ul>
                            <li>Data Visualization</li>
                            <li>Model 1 Data Visualization</li>
                            <li>Model 2 Data Visualization</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Manushree</td>
                    <td>
                        <ul>
                            <li>Selection, Implementation, and Analysis of Bagging Model (Model 1)</li>
                            <li>Update Github Page</li>
                        </ul>
                    </td>
                </tr>
                <tr>
                    <td>Tanmay</td>
                    <td>
                        <ul>
                            <li>Selection, Implementation, and Analysis of Random Forest Model (Model 2)</li>
                            <li>Update Github Page</li>
                        </ul>
                    </td>
                </tr>
            </tbody>
        </table>
    </div>

    <div class="section">
        <h2>Project Award Eligibility</h2>
        <p>Our team would like to opt-in to be considered for the "Outstanding Project" award. We understand that the final project website and contributors of winning projects will be featured on the course website.</p>
    </div>

</body>
</html>